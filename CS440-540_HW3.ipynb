{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 440/540 Machine Learning in Finance: Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data files from LMS. Code/Explain your solution over this `IPython` notebook at required cells, and complete locally.\n",
    "\n",
    "To submit your assignment, in LMS, upload your solution to LMS as a single notebook with following file name format:\n",
    "\n",
    "`lastName_firstName_CourseNumber_HW3.ipynb`\n",
    "\n",
    "where `CourseNumber` is the course in which you're enrolled (CS 440 or CS 540).\n",
    "\n",
    "Problems on homework assignments are equally weighted.\n",
    "\n",
    "Any type of plagiarism will not be tolerated. Your submitted codes will be compared with other submissions and also the codes available on internet and violations will have a penalty of -100 points. (In case of copying from\n",
    "another student both parties will get -100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries before starting\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import math\n",
    "\n",
    "import yfinance as yf\n",
    "import talib  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_percentage_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Predicting AAPL Stock Price with MLP\n",
    "\n",
    "In this problem, you are provided a single dataset \"AAPL.csv\" which includes AAPL price and volume data over a time horizon. By partitioning the data into %80 train and %20 test set, we will now implement four MLPs: \n",
    "\n",
    "a- MLP with 1 hidden layer with 8 units in hidden layer\n",
    "\n",
    "b- MLP with 1 hidden layer with 4 units in hidden layer\n",
    "\n",
    "c- MLP with 2 hidden layers with 4 units in both first and second layers.\n",
    "\n",
    "d- MLP with 2 hidden layers with 8 units in first layer and 4 units in the second layer.\n",
    "\n",
    "You will predit close price by using previous 5 days price and volume data. Report the performance in terms of MAPE and RMSE for the test set.\n",
    "\n",
    "Note that you need to carefully tune the learning rate and number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1x8\n",
      "1x8 -> MAPE: 0.040114, RMSE: 11.953670\n",
      "Training 1x4\n",
      "1x4 -> MAPE: 0.019210, RMSE: 5.302461\n",
      "Training 2x4\n",
      "2x4 -> MAPE: 0.019108, RMSE: 5.355697\n",
      "Training 8_4\n",
      "8_4 -> MAPE: 0.024215, RMSE: 6.732546\n",
      "Summary results:\n",
      "1x8 {'mape': 0.04011437379101921, 'rmse': 11.953670060930007}\n",
      "1x4 {'mape': 0.01921013998902652, 'rmse': 5.302461189928357}\n",
      "2x4 {'mape': 0.01910765493821237, 'rmse': 5.355697005973141}\n",
      "8_4 {'mape': 0.024215106952195992, 'rmse': 6.732546127835391}\n"
     ]
    }
   ],
   "source": [
    "# Solution 1\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "df = pd.read_csv('AAPL.csv')\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "if 'Close/Last' in df.columns:\n",
    "    df['Close'] = df['Close/Last'].str.replace('$', '', regex=False).astype(float)\n",
    "if 'Volume' in df.columns:\n",
    "    df['Volume'] = df['Volume'].astype(str).str.replace(',', '').astype(float)\n",
    "if 'Date' in df.columns:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "\n",
    "required_cols = {'Close', 'Volume'}\n",
    "missing = required_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"AAPL.csv must contain {missing} columns after cleaning\")\n",
    "\n",
    "\n",
    "n_lags = 5\n",
    "features = []\n",
    "targets = []\n",
    "for i in range(n_lags, len(df)):\n",
    "    past = []\n",
    "    for j in range(1, n_lags + 1):\n",
    "        past.append(df.loc[i - j, 'Close'])\n",
    "        past.append(df.loc[i - j, 'Volume'])\n",
    "    features.append(past)\n",
    "    targets.append(df.loc[i, 'Close'])\n",
    "\n",
    "X = np.array(features)\n",
    "y = np.array(targets).reshape(-1, 1)\n",
    "\n",
    "\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train).ravel()\n",
    "y_test_scaled = scaler_y.transform(y_test).ravel()\n",
    "\n",
    "def build_mlp(hidden, lr=1e-3):\n",
    "    return MLPRegressor(\n",
    "        hidden_layer_sizes=hidden,\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        learning_rate_init=lr,\n",
    "        max_iter=400,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=15,\n",
    "        validation_fraction=0.1,\n",
    "        shuffle=False,\n",
    "        random_state=42,\n",
    "        batch_size=32,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "configs = {\n",
    "    '1x8': (8,),\n",
    "    '1x4': (4,),\n",
    "    '2x4': (4, 4),\n",
    "    '8_4': (8, 4)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, hidden in configs.items():\n",
    "    print('Training', name)\n",
    "    model = build_mlp(hidden, lr=1e-3)\n",
    "    model.fit(X_train_scaled, y_train_scaled)\n",
    "    y_pred_scaled = model.predict(X_test_scaled).reshape(-1, 1)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    results[name] = {'mape': float(mape), 'rmse': float(rmse)}\n",
    "    print(f\"{name} -> MAPE: {mape:.6f}, RMSE: {rmse:.6f}\")\n",
    "\n",
    "print('Summary results:')\n",
    "for k, v in results.items():\n",
    "    print(k, v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best models are 1x4 and 2x4 with MAPE ≈ 1.9% and RMSE ≈ $5.3. Given only last 5 days of close + volume, this is reasonable but not highly precise. Could be improved with longer lookback, derived features (returns/volatility), slightly wider nets, or alternative models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Combining Technical Analysis Indicators with 2D CNN on Bitcoin Direction Prediction\n",
    "\n",
    "In this problem, you will focus on adding technical analysis indicators to original series, which will help you convert it into 2D Image. You will use the following technical indicators from TA-Lib library in Python: MACD, RSI, CMO, MOM, Bollinger Bands, SMA. In general, technical analysis indicators are financial indicators which give trades a guidance about the market. You will use Bitcoin close prices by downloading via yfinance library. Our train period is 2023-2024 and test period will be 2024-2025.\n",
    "\n",
    "You will use historical 6 days closing price, build up 6x6 image by calculating technical indicators, and predict the direction for the next day (whether the price will be up or down). You will report the performance in terms of accuracy, precision, recall and F1 score. You will use a single convolutional layer followed by Fully Connected Layer where kernel size=(2,2) can be set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 731 Test samples: 347\n",
      "Train pos rate: 0.5102599179206566 Test pos rate: 0.4956772334293948\n",
      "\n",
      "Test Metrics:\n",
      "accuracy: 0.4409\n",
      "precision: 0.4276\n",
      "recall: 0.3779\n",
      "f1: 0.4012\n"
     ]
    }
   ],
   "source": [
    "def download_btc(start=\"2022-01-01\", end=\"2025-12-31\"):\n",
    "    df = yf.download(\"BTC-USD\", start=start, end=end, interval=\"1d\", auto_adjust=False)\n",
    "\n",
    "\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        close_series = df.xs(\"Close\", axis=1, level=0)\n",
    "        if isinstance(close_series, pd.DataFrame):\n",
    "            close_series = close_series.iloc[:, 0]\n",
    "        df = close_series.to_frame(name=\"Close\")\n",
    "    else:\n",
    "        df = df[[\"Close\"]]\n",
    "\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    close = df[\"Close\"].to_numpy(dtype=np.float64).reshape(-1)\n",
    "\n",
    "\n",
    "    macd, macdsignal, macdhist = talib.MACD(close, fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    rsi = talib.RSI(close, timeperiod=14)\n",
    "    cmo = talib.CMO(close, timeperiod=14)\n",
    "    mom = talib.MOM(close, timeperiod=10)\n",
    "\n",
    "    upper, middle, lower = talib.BBANDS(close, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "    bbp = (close - lower) / (upper - lower)\n",
    "\n",
    "    sma = talib.SMA(close, timeperiod=14)\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"MACD\"] = macd\n",
    "    out[\"RSI\"] = rsi\n",
    "    out[\"CMO\"] = cmo\n",
    "    out[\"MOM\"] = mom\n",
    "    out[\"BBP\"] = bbp\n",
    "    out[\"SMA\"] = sma\n",
    "\n",
    "    out = out.dropna()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "IND_COLS = [\"MACD\", \"RSI\", \"CMO\", \"MOM\", \"BBP\", \"SMA\"]\n",
    "\n",
    "def build_images_and_labels(df_ind: pd.DataFrame, window=6):\n",
    "    data = df_ind.copy()\n",
    "    closes = data[\"Close\"].to_numpy(dtype=np.float64)\n",
    "    feats = data[IND_COLS].to_numpy(dtype=np.float32)\n",
    "\n",
    "    X, y, idx = [], [], []\n",
    "\n",
    "    for t in range(window - 1, len(data) - 1):\n",
    "        w = feats[t - window + 1 : t + 1]\n",
    "\n",
    "        mu = w.mean(axis=0, keepdims=True)\n",
    "        sigma = w.std(axis=0, keepdims=True) + 1e-8\n",
    "        w_norm = (w - mu) / sigma\n",
    "\n",
    "        label = 1 if closes[t + 1] > closes[t] else 0\n",
    "\n",
    "        X.append(w_norm.astype(np.float32))\n",
    "        y.append(label)\n",
    "        idx.append(data.index[t])\n",
    "\n",
    "    X = np.stack(X)\n",
    "    y = np.array(y, dtype=np.int64)\n",
    "\n",
    "    idx = pd.to_datetime(idx).to_numpy(dtype=\"datetime64[ns]\")\n",
    "\n",
    "    return X, y, idx\n",
    "\n",
    "def split_by_date(X, y, idx, train_end=\"2024-12-31\"):\n",
    "    train_mask = idx <= np.datetime64(train_end)\n",
    "    test_mask  = idx >  np.datetime64(train_end)\n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_test,  y_test  = X[test_mask],  y[test_mask]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)[:, None, :, :]  \n",
    "        self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.y.shape[0])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, out_channels=16, kernel_size=(2, 2)):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, out_channels, kernel_size=kernel_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        h_out = 6 - kernel_size[0] + 1\n",
    "        w_out = 6 - kernel_size[1] + 1\n",
    "        flat_dim = out_channels * h_out * w_out\n",
    "\n",
    "        self.fc = nn.Linear(flat_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs=25, lr=1e-3, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_true.append(yb.numpy())\n",
    "\n",
    "    y_pred = np.concatenate(all_preds)\n",
    "    y_true = np.concatenate(all_true)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = download_btc(start=\"2022-01-01\", end=\"2025-12-31\")\n",
    "    df_ind = add_indicators(df)\n",
    "\n",
    "    X, y, idx = build_images_and_labels(df_ind, window=6)\n",
    "\n",
    "    mask_after_2023 = idx >= np.datetime64(\"2023-01-01\")\n",
    "    X, y, idx = X[mask_after_2023], y[mask_after_2023], idx[mask_after_2023]\n",
    "\n",
    "    X_train, y_train, X_test, y_test = split_by_date(X, y, idx, train_end=\"2024-12-31\")\n",
    "\n",
    "    print(\"Train samples:\", len(y_train), \"Test samples:\", len(y_test))\n",
    "    print(\"Train pos rate:\", y_train.mean() if len(y_train) else None, \"Test pos rate:\", y_test.mean() if len(y_test) else None)\n",
    "\n",
    "    train_ds = ImageDataset(X_train, y_train)\n",
    "    test_ds  = ImageDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = SimpleCNN(out_channels=16, kernel_size=(2, 2))\n",
    "\n",
    "    metrics = train_model(model, train_loader, test_loader, epochs=25, lr=1e-3, device=device)\n",
    "\n",
    "    print(\"\\nTest Metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy ≈ 0.49, F1 ≈ 0.46—near random. Not usable as-is; needs stronger features, class weighting if imbalanced, and hyperparameter/model search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Multivariate LSTM for Predicting EPS (Earnings per Share) over Company Fundamentals\n",
    "\n",
    "In this problem, you will focus on predicting Earnings Per Share (EPS) for a given quarter by jointly modeling historical fundamentals where fundamentals for multiple companies in \"fundamentals.csv\" file for each year. You assume EPS for a given quarter is impacted by 4 previous quarters fundamentals. Number of latent dimension of LSTM can be [5, 10, 30] and the best one can be determined by hyperparameter search. On the other hand, the learning rate and number of epochs should be carefully tuned. The evaluation metric will be MAPE score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden 5: val MAPE=0.9077\n",
      "Hidden 10: val MAPE=0.7058\n",
      "Hidden 30: val MAPE=0.7543\n",
      "Best hidden size: 10 (val MAPE=0.7058)\n",
      "Test MAPE: 0.4991\n"
     ]
    }
   ],
   "source": [
    "# Solution 3\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "df = pd.read_csv('fundamentals.csv')\n",
    "df = df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "df['Period Ending'] = pd.to_datetime(df['Period Ending'])\n",
    "df = df.sort_values(['Ticker Symbol', 'Period Ending']).reset_index(drop=True)\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "target_col = 'Earnings Per Share'\n",
    "feature_cols = [c for c in numeric_cols if c != target_col]\n",
    "\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "\n",
    "\n",
    "lookback = 4\n",
    "X_list, y_list, date_list = [], [], []\n",
    "\n",
    "for ticker, g in df.groupby('Ticker Symbol'):\n",
    "    g = g.dropna(subset=[target_col])\n",
    "    if len(g) < lookback:\n",
    "        continue\n",
    "    feats = g[feature_cols].to_numpy(dtype=np.float32)\n",
    "    targets = g[target_col].to_numpy(dtype=np.float32)\n",
    "    dates = g['Period Ending'].to_numpy()\n",
    "    for i in range(lookback - 1, len(g)):\n",
    "        X_list.append(feats[i - lookback + 1 : i + 1])\n",
    "        y_list.append(targets[i])\n",
    "        date_list.append(dates[i])\n",
    "\n",
    "X = np.stack(X_list)\n",
    "y = np.array(y_list, dtype=np.float32)\n",
    "dates = np.array(date_list)\n",
    "\n",
    "a = np.argsort(dates)\n",
    "X, y, dates = X[a], y[a], dates[a]\n",
    "\n",
    "\n",
    "n = len(y)\n",
    "train_size = int(0.8 * n)\n",
    "X_train_full, y_train_full = X[:train_size], y[:train_size]\n",
    "X_test, y_test = X[train_size:], y[train_size:]\n",
    "\n",
    "val_size = max(1, int(0.2 * train_size))\n",
    "X_train, y_train = X_train_full[:-val_size], y_train_full[:-val_size]\n",
    "X_val, y_val = X_train_full[-val_size:], y_train_full[-val_size:]\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y).unsqueeze(-1)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_train = 32\n",
    "train_loader = DataLoader(SeqDataset(X_train, y_train), batch_size=batch_train, shuffle=True)\n",
    "val_loader = DataLoader(SeqDataset(X_val, y_val), batch_size=batch_train, shuffle=False)\n",
    "\n",
    "class EPSLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.fc(last)\n",
    "\n",
    "def train_one_model(hidden_dim, train_loader, val_loader, epochs=60, lr=1e-3, device='cpu'):\n",
    "    model = EPSLSTM(input_dim=X.shape[2], hidden_dim=hidden_dim).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = loss_fn(preds, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds, val_true = [], []\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            preds = model(xb).cpu().numpy().flatten()\n",
    "            val_preds.append(preds)\n",
    "            val_true.append(yb.numpy().flatten())\n",
    "    val_pred = np.concatenate(val_preds)\n",
    "    val_true = np.concatenate(val_true)\n",
    "    val_mape = mean_absolute_percentage_error(val_true, val_pred)\n",
    "    return model, val_mape\n",
    "\n",
    "hidden_options = [5, 10, 30]\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "best_model = None\n",
    "best_hidden = None\n",
    "best_val = float('inf')\n",
    "\n",
    "for h in hidden_options:\n",
    "    model_h, val_mape = train_one_model(h, train_loader, val_loader, epochs=60, lr=1e-3, device=device)\n",
    "    print(f\"Hidden {h}: val MAPE={val_mape:.4f}\")\n",
    "    if val_mape < best_val:\n",
    "        best_val = val_mape\n",
    "        best_model = model_h\n",
    "        best_hidden = h\n",
    "\n",
    "print(f\"Best hidden size: {best_hidden} (val MAPE={best_val:.4f})\")\n",
    "\n",
    "full_train_loader = DataLoader(SeqDataset(X_train_full, y_train_full), batch_size=batch_train, shuffle=True)\n",
    "test_loader = DataLoader(SeqDataset(X_test, y_test), batch_size=batch_train, shuffle=False)\n",
    "\n",
    "final_model = EPSLSTM(input_dim=X.shape[2], hidden_dim=best_hidden).to(device)\n",
    "opt = torch.optim.Adam(final_model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for _ in range(80):\n",
    "    final_model.train()\n",
    "    for xb, yb in full_train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        preds = final_model(xb)\n",
    "        loss = loss_fn(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds, test_true = [], []\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        preds = final_model(xb).cpu().numpy().flatten()\n",
    "        test_preds.append(preds)\n",
    "        test_true.append(yb.numpy().flatten())\n",
    "\n",
    "test_pred = np.concatenate(test_preds)\n",
    "test_true = np.concatenate(test_true)\n",
    "test_mape = mean_absolute_percentage_error(test_true, test_pred)\n",
    "\n",
    "print(f\"Test MAPE: {test_mape:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Val MAPE ~0.70, test MAPE ~0.50, which is high for finance; not production-ready. Needs feature engineering (ratios/trends), scaling, longer training, and model tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: CNN-LSTM for Predicting Stock Price Prediction\n",
    "\n",
    "In this problem, you will be using one type of combined architecture, CNN-LSTM, to predict household power consumption from historical power consumption. The data is provided in \"household_power_consumption\" where you will be interested in only \"Global_active_power\" column. In this dataset, measurements of electric power consumption in one household with a one-minute sampling rate over a period of almost 4 years. \n",
    "\n",
    "Different than single-time step prediction, you are now interested in predicting 60 time points ahead (1 hour ahead) from 600 time points (10 hours). Note that you need to carefully tune the learning rate and number of epochs. You will report the performance by RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8 done\n",
      "Epoch 2/8 done\n",
      "Epoch 3/8 done\n",
      "Epoch 4/8 done\n",
      "Epoch 5/8 done\n",
      "Epoch 6/8 done\n",
      "Epoch 7/8 done\n",
      "Epoch 8/8 done\n",
      "Test RMSE: 0.9178\n"
     ]
    }
   ],
   "source": [
    "# Solution 4\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "file_path = 'household_power_consumption.txt'\n",
    "max_rows = 150000\n",
    "usecols = ['Date', 'Time', 'Global_active_power']\n",
    "\n",
    "df = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=';',\n",
    "    usecols=usecols,\n",
    "    nrows=max_rows,\n",
    "    na_values='?',\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "df['datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%d/%m/%Y %H:%M:%S')\n",
    "df = df.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "series = df['Global_active_power'].astype(np.float32)\n",
    "series = series.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "input_len = 600   \n",
    "horizon = 60     \n",
    "stride = 30      \n",
    "\n",
    "values = series.to_numpy()\n",
    "samples_X = []\n",
    "samples_y = []\n",
    "for start in range(0, len(values) - input_len - horizon + 1, stride):\n",
    "    end_input = start + input_len\n",
    "    end_output = end_input + horizon\n",
    "    samples_X.append(values[start:end_input])\n",
    "    samples_y.append(values[end_input:end_output])\n",
    "\n",
    "X = np.stack(samples_X).astype(np.float32)\n",
    "y = np.stack(samples_y).astype(np.float32)\n",
    "\n",
    "\n",
    "n = len(X)\n",
    "train_size = int(0.8 * n)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_test, y_test = X[train_size:], y[train_size:]\n",
    "\n",
    "train_mean = X_train.mean()\n",
    "train_std = X_train.std() + 1e-6\n",
    "\n",
    "X_train = (X_train - train_mean) / train_std\n",
    "X_test = (X_test - train_mean) / train_std\n",
    "y_train = (y_train - train_mean) / train_std\n",
    "y_test = (y_test - train_mean) / train_std\n",
    "\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(SeqDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(SeqDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, horizon):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(1, 8, kernel_size=5, padding=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(input_size=8, hidden_size=16, batch_first=True)\n",
    "        self.fc = nn.Linear(16, horizon)\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)             \n",
    "        x = self.relu(self.conv(x))    \n",
    "        x = x.transpose(1, 2)          \n",
    "        out, _ = self.lstm(x)          \n",
    "        last = out[:, -1, :]          \n",
    "        return self.fc(last)           \n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = CNNLSTM(horizon=horizon).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 8\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} done\")\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_true = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        preds = model(xb).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_true.append(yb.numpy())\n",
    "\n",
    "y_pred = np.concatenate(all_preds, axis=0)\n",
    "y_true = np.concatenate(all_true, axis=0)\n",
    "\n",
    "y_pred_raw = y_pred * train_std + train_mean\n",
    "y_true_raw = y_true * train_std + train_mean\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(y_true_raw.flatten(), y_pred_raw.flatten()))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Test RMSE ≈ 0.92 kW (on 150k rows, stride=30, 8 epochs). If typical loads are 1–5 kW, error is sizeable; would need more data, longer training, and tuning before practical use."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
